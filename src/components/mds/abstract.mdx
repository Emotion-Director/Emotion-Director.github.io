# Abstract

Text-to-image (T2I) generation based on diffusion models has demonstrated impressive performance, motivating exploration into diverse and specialized applications. Despite the growing potential in creative marketing, **emotion-oriented T2I** \(E-T2I\) remains challenging for leading models due to data scarcity and semantic abstractness. To this end, we propose Emotion-Director, a framework that employs multi-modal prompts (MMP) as a core principle to construct a dedicated dataset and a tailored model for E-T2I generation. The dataset comprises 80K textual prompts and 20K preference images. Textual prompts are generated by our proposed **MMP-Agent** system, which integrates MMP-driven analysis and feedback to focus on concrete emotional concepts. Subsequently, the images generated from these prompts are filtered through automated scoring and human annotation to form high-quality preference pairs. Next, we propose **MMP-Diffusion** for image generation, which employs MMP in the cross-attention mechanism. The textual prompt enriches concrete visual concepts, and the visual prompt evokes an abstract visual atmosphere. This synergistic guidance provides substantial information for emotional expression. Extensive qualitative and quantitative experiments demonstrate the superiority of Emotion-Director for E-T2I generation.